# Linear modeling 

Recall from the foundations chapter that 
the way we will conduct data analysis is as follows. 

  - Given data, specify a *likelihood function*.
  - Specify *prior distributions* for model parameters.
  - Using software, derive *marginal posterior distributions* for parameters given likelihood function and prior density.
  - Simulate parameters to get *samples from posterior distributions* of parameters using some *Markov Chain Monte Carlo (MCMC) sampling algorithm*.
  - Evaluate whether model makes sense, using *model convergence* diagnostics, 
  fake-data simulation, *prior predictive* and *posterior predictive* checks, and (if you want to claim a discovery) calibrating true and false discovery rates.
  - Summarize *posterior distributions* of parameter samples and make your scientific decision.

We will now work through some specific examples to illustrate how the data analysis process works.

## Example 1: A single subject pressing a button repeatedly \label{sec:first}

As a first example, we will fit a simple linear model to some reaction time data.

First, load the brms package. For an introduction to this package, see @brms. 

```{r}
library(brms)
```

The file \verb|button_press.dat| contains data of a subject pressing the space bar without reading in a self-paced reading experiment.

### Preprocessing of the data

```{r, reading_noreading}
noreading_data <- read.table(header = F,"data/button_press.dat",
                             encoding="latin1")
noreading_data <- noreading_data[c("V2","V3","V5","V6","V8")]
colnames(noreading_data) <- c("type","item","wordn","word","RT")
tail(noreading_data)
summary(noreading_data$RT)
class(noreading_data)
```

### Visualizing the data

It is a good idea to look at the distribution of the data before doing anything else. See Figure \ref{fig:m1visualize}.

```{r fig.cap="\\label{fig:m1visualize}Visualizing the data."}
plot(density(noreading_data$RT),
     main="Button-press data",xlab="RT")
```

The data looks a bit skewed, but we ignore this for the moment.

### Probability model

Let's model the data with the following assumptions:

- There is a true underlying time, $\mu$, that the participant needs to press the space-bar.
- There is some noise in this process.
- The noise is normally distributed (this assumption is questionable given the skew).

This means that the likelihood for each observation $i$ will be:

\begin{equation}
\begin{aligned}
y_i \sim Normal(\mu, \sigma)
\end{aligned}
\end{equation}

where $i =1 \ldots N$.

This is just the simple linear model:

$y = \mu + \varepsilon \hbox{ where } \varepsilon \sim Normal(0,\sigma)$

We are going to use the following priors for the two parameters in this model:
\begin{equation}
\begin{aligned}
\mu &\sim Normal(0, 2000) \\
\sigma &\sim Normal(0, 500) \text{ truncated so that } \sigma > 0
\end{aligned}
\end{equation}

The prior for $\mu$ is encoding the following information: The model expects
that the underlying time could be both positive and negative, and given that
the scale of the prior (in this case the standard deviation of the normal
distribution) is 2000, we are $\approx 68\%$ certain that the true value would
be between 2000 ms and -2000 ms and $\approx 95\%$ certain that it would be
between -4000 ms and 4000 ms (two standard deviations away from zero). But we
obviously know that the time can't be negative! So we have more prior
information than what we are using for informing the model. We'll discuss this
later. Regarding the prior for $\sigma$: It must be positive, and we are
$\approx 68\%$ certain that the true value would be between 0 ms and 500 ms
and $\approx 95\%$ certain that it would be between 0 ms and 1000 ms.

### Implementing model in brms

This model is expressed in brms in the following way. First, define the priors:

```{r}
priors <- c(set_prior("normal(0, 2000)", 
                      class = "Intercept"),
            set_prior("normal(0, 500)", 
                      class = "sigma"))
```

Then, define the generative process assumed:

```{r cache=TRUE,warning=FALSE,message=FALSE,results="hide"}
m1<-brm(RT~1,noreading_data,prior = priors,
       iter = 2000,
       warmup = 1000,
       chains = 4,
       family = gaussian(), 
       control = list(adapt_delta = 0.99))
```

\begin{enumerate}
\item
The term \texttt{family~=~gaussian()} makes explicit the underlying likelihood function that is implicit in \texttt{lme4}. Other linking functions are possible, exactly as in the \texttt{glmer} function in \texttt{lme4}.
\item
The term \texttt{prior} takes as argument the list of priors we defined in Listing~\ref{fig:priors}. Although this specification of priors is optional, the  researcher should always explicitly specify each prior. Otherwise, \texttt{brms} will define a prior by default, which may or may not be appropriate for the research area. We return to this point below.
\item 
The term \texttt{iter} refers to the number of iterations  that the sampler makes to sample from the posterior distribution of each parameter (by default 2000).
\item 
The term \texttt{warmup} refers to the number of iterations from the start of sampling that are eventually discarded (by default half of \texttt{iter}).
\item 
The term 
\texttt{chains} refers to the number of independent runs for sampling (by default four).
\item
The term \texttt{control} refers to some optional control parameters for the sampler, such as \texttt{adapt\_delta}, \texttt{max\_treedepth}, and so forth.
\end{enumerate}


### Summarizing the posteriors, and convergence diagnostics

The summary displayed below show summary statistics over the marginal posterior distributions of the parameters in the model. The summary shows posterior means,  standard  deviations  (`sd`),  quantiles,  Monte
Carlo standard errors (`se_mean`), split Rhats, and effective sample sizes (`n_eff`).  

The summaries
are computed after removing the warmup  and merging together all chains. Notice that the `se_mean` is unrelated to the `se` of an estimate in the frequentist model.  


```{r}
summary(m1)
```

A graphical summary of posterior distributions of model m1 is shown in Figure \ref{fig:m1stanplot}:

```{r fig.cap="\\label{fig:m1stanplot}Posterior distributions of the parameters in model m1."}
stanplot(m1,type="hist")
```

The trace plots in Figure \ref{fig:m1traceplot} show how well the four chains are mixing:

```{r fig.cap="\\label{fig:m1traceplot}Trace plots in model m1."}
stanplot(m1,type="trace")
```

### Validating model using fake-data simulation

An important way to check that the model can recover the true parameters is by using fake-data simulation [@GelmanEtAl2014].

```{r, fake_data}
set.seed(123)
N <- 500
true_mu <- 400
true_sigma <- 125
RT <- rnorm(N, true_mu, true_sigma)

RT <- round(RT) 
fake_data <- data.frame(RT=RT)
```


```{r fit_fake, message=FALSE, warning=FALSE, results="hide",cache=TRUE}

m1_fake<-brm(RT~1,fake_data,prior = priors,
       iter = 2000, chains = 4,family = gaussian(), 
       control = list(adapt_delta = 0.99))
```

The model recovers the parameters:

```{r}
summary(m1_fake)
```
You should also check through trace plots that the chains are mixing properly, and you should also plot the posterior distributions of the parameters.

## MCMC diagnostics: Convergence problems and Stan warnings

Because we are using numerical methods to sample from the posterior distributions, we need to make sure that the model is able to sample from the posterior.

The most important checks or MCMC diagnostics are the following:


* The chains should look like a straight "fat hairy caterpillar": the chains should
  bounce around the same values and with the same variance.
* The potential scale reduction factors, $\hat{R}$s, of the parameters should
  be close to one (as a rule of thumb less than $1.1$).  This indicates that
  the chains have mixed and they are traversing the same distribution.
  $\hat{R}$s are printed in the summary in the column `Rhat` [see
  section 11.4 of @GelmanEtAl2014].
* The effective sample size, $n_{eff}$ should be large enough. The effective
  sample size is an estimate of the number of independent draws from the
  posterior distribution. Since the samples are not independent, $n_{eff}$
  will generally be smaller than the total number of samples, $N$.  How large
  $n_{eff}$ should be depends on the summary statistics that we want to use.
  But as a rule of thumb, $n_{eff}/N > 0.1$.
* There are no (important) warnings, such as, divergent transitions, Bayesian
  fraction of missing information (BFMI) that was too low, etc. These warning may
  indicate that the sampler is not adequately exploring  the parameter space.
  See also http://mc-stan.org/misc/warnings.html

For useful graphical checks see https://cran.r-project.org/web/packages/bayesplot/vignettes/MCMC-diagnostics.html

These issues **should not be ignored**! See the Appendix \ref{sec:Appendix} for some troubleshooting ideas to solve them.

## The posterior distribution of $\mu$vs the distribution of RTs

```{r}
hist(RT)
stanplot(m1_fake,type="hist",pars="b_Intercept")
```

We are assuming that there's a true underlying time it takes to press the space bar, $\mu$, and there is normally distributed noise that generates the different RTs. This is encoded in our likelihood by assuming that RTs are distributed with an unknown true mean $\mu$ (and an unknown standard deviation $\sigma$). The objective of the Bayesian model is to learn about the possible values of $\mu$, or in other words, to get a distribution that encodes what we know about the true mean of the distribution of RTs (and also another distribution that encodes what we know about true standard deviation, $\sigma$, of the distribution of RTs.)

*Given our data, what can we learn from the posterior distribution of $\mu$?*

Our model allows us to have answers to questions such as:

**What is the probability that the underlying value of the mindless press of
the space bar would be over, say 170 ms?**

**Answer**:


**Which range of values contains a specified amount of probability?**


This type of interval is also known as a *credible interval*. 

A credible interval demarcates the range within which we can be certain with a certain probability that the "true value" of a parameter lies given the data and the model.
This is very different from the frequentist confidence interval! See for example, @HoekstraEtAl2014 and @MoreyEtAl2015.

The percentile interval is a type of credible interval (the most common one), where we assign equal probability mass to each tail. We generally report 95% credible intervals. But we can extract any interval, a 73% interval, for example, leaves `r (1.00-.73)/2*100`% of the probability mass on each tail, and we can calculate it like this:

to-do

### Influence of priors and sensitivity analysis

Everything was normally distributed in our example (or truncated normal), but the fact that we assumed that RTs were normally distributed is completely unrelated to our (truncated) normally distributed priors. Let's try a  uniform prior on $\mu$  with a low boundary of 0 and a high boundary of 5000. Here, we assume that every value between 0 and 5000 is equally likely. In general, this is a bad idea for two reasons: (i) it is computationally expensive (the sampler has a larger parameter space to search), and (ii) it is providing information that we know is not sensible (every value between 0 and 5000 cannot be equally likely). But in our very simple example these priors will give use the same posterior as with the normal priors.


\begin{equation}
\begin{aligned}
\mu &\sim Uniform(0,5000) \\
\sigma &\sim Uniform(0, 5000) 
\end{aligned}
\end{equation}

```{r}
priors <- c(set_prior("uniform(0, 5000)", 
                      class = "Intercept"),
            set_prior("normal(0, 500)", 
                      class = "sigma"))
```

```{r cache=TRUE}
m<-brm(RT~1,noreading_data,prior = priors,
       iter = 2000, chains = 4,family = gaussian(), 
       control = list(adapt_delta = 0.99))
summary(m)
```

In general, we don't want our priors to have too much influence on our
posterior. This is unless we have *very* good reasons for having informative
priors, such as a very small sample and a lot of prior information; an example
would be if we have data from an impaired population, which makes it hard to
increase our sample size. 

We usually center the priors on 0 and we let the
likelihood dominate in determining the posterior. This type of
prior is called *weakly informative prior*. Notice that a uniform prior is
not a weakly informative prior, it assumes that every value is equally
likely, zero is as likely as 5000. 

You should always do a 
*sensitivity analysis* to check how influential the prior is: try different priors and verify that the posterior
doesn't change drastically [for a published  example, see
@VasishthetalPLoSOne2013]. See also Exercise \ref{ex:firstpriors}.

## A slightly more interesting analysis of the small experiment \label{sec:moreinter}

More realistically, we might have run the small experiment to know whether the
participant tended to speedup (practice effect) or slowdown (fatigue effect) while 
pressing the space bar. 


### Preprocessing the data

We need to have data about the number of times the space bar was
pressed for each observation, and add it to our list. It's a good idea to
center the number of presses (a covariate) to have a
clearer interpretation of the intercept. In general, centering predictors is
always a good idea, for interpretability and for computational reasons.

```{r, reading_noreading_sb}
# We create the new column in the data frame
noreading_data$presses <- 1:nrow(noreading_data)
# We center the column
noreading_data$c_presses <- noreading_data$presses - mean(noreading_data$presses)
```

### Probability model


Our model changes, because we have a new parameter. 

\begin{equation}
RT_i \sim Normal(\alpha + presses_i \cdot \beta,\sigma)
\end{equation}

where $i =1 \ldots N$


And we are going to use the following priors:

\begin{equation}
\begin{aligned}
\alpha &\sim Normal(0, 2000) \\
\beta &\sim Normal(0, 500) \\
\sigma &\sim Normal(0, 500) \text{ truncated so that } \sigma > 0 \\
\end{aligned}
\end{equation}


We are basically fitting a linear model, $\alpha$ represents the intercept (namely, the grand mean of the RTs), and $\beta$ represents the slope. Which information are the priors encoding? Does it make sense?



We'll write this in brms as follows.

```{r}
priors <- c(set_prior("normal(0, 2000)", 
                      class = "Intercept"),
            set_prior("normal(0, 500)", 
                      class = "b",
                      coef="presses"),
            set_prior("normal(0, 500)", 
                      class = "sigma"))
```

```{r cache=TRUE}
m<-brm(RT~1+presses,noreading_data,prior = priors,
       iter = 2000, chains = 4,family = gaussian(), 
       control = list(adapt_delta = 0.99))
summary(m)
```

### Summarizing the posterior and inference

How can we answer our research question? What is the effect of pressing the
bar on the participant's reaction time?


We'll need to examine what happens with $\beta$. The summary gives us important information:

```{r}
m_post_samp_b <- posterior_samples(m, "^b")
beta_samples <- m_post_samp_b$b_presses 
beta_mean<-mean(beta_samples)
quantiles_beta <- quantile(beta_samples,prob=c(0.025,0.975))
beta_low<-quantiles_beta[1]
beta_high<-quantiles_beta[2]
```

We can learn that the most likely values of $\beta$ will be around the mean of the posterior `r beta_mean`, and we can be 95% certain that the true value of $\beta$ *given the model and the data* lies between `r beta_low` and `r beta_high`. 

We see that as the number of times the space bar is pressed increases, the participant becomes slower. If we want to know how  likely it is that the participant was slower rather than faster, we can examine the proportion of samples above zero:

```{r beta_samples}
mean(beta_samples > 0)
```

We would report this in a paper as $\hat\beta = `r beta_mean`$, 95% CrI = $[ `r beta_low` , `r beta_high` ]$, $P(\beta >0)  \approx `r mean(beta_samples > 0)`$. 
Plotting the posterior as a histogram is always a good idea.

Can we really conclude that there is a fatigue effect? It depends on how much
we expect the fatigue to affect the RTs. Here we see that only after 100
button presses, we'll see a slowdown of 9 ms on average ($0.09 \times 100$). We
will need to think whether the size of this effect make sense considering the
previous literature. Sometimes this requires a meta-analysis. See @JaegerEtAl2017 for an example, and the use of this prior knowledge in @NicenboimEtAl2016NIG.



### Posterior predictive checks

Let's say we know that our model is working as expected, since we already used
fake data to test the recovery of the parameters. How do we know if our model
is "good"?

We will examine the *descriptive adequacy* of the models [@ShiffrinEtAl2008;
@GelmanEtAl2014, Chapter 6]: the observed data should look plausible under the
*posterior predictive distribution*. The posterior predictive distribution is
composed of one dataset for each sample from the posterior. (So it will
generate as many datasets as iterations we have after the warm-up.)  Achieving
descriptive adequacy means that the current data could have been predicted
by the model. Passing a test of descriptive adequacy is not strong evidence
in favor of a model, but a major failure in descriptive adequacy can be
interpreted as strong evidence against a model [@ShiffrinEtAl2008].


To do posterior predictive checks for our last example, we need to do:

```{r}
pp_check(m, nsamples = 100)+
  theme(text = element_text(size=16),
        legend.text=element_text(size=16))
```




We'll use the values generated by our model to verify whether the general
shape of the actual distribution matches the distributions from some of the
generated datasets. Let's compare the real data against 100 of the predicted 4000
datasets:


*Is the simulated data similar to the real data?*

Our dataset seems to be more skewed to the right than  our predicted
datasets. This is not too surprising, we assumed that the likelihood was a
normal distribution, but latencies are not very normal-like, they can't be
negative and can be arbitrarily long.


### A better probability model using the log-normal distribution

Since we know that the latencies shouldn't be normally distributed, we can choose a more realistic distribution for the likelihood. A good candidate is the log-normal distribution since a variable (such as time) that is log-normally distributed takes only positive real values. 

If $Y$ is log-normally distributed, this means that $log(Y)$ is normally distributed.^[In fact, $log_e(Y)$ or $ln(Y)$, but since it is the most popular logarithm in statistics we'll write it as just $log()$] Something important to notice is that the log-normal distribution is defined using again $\mu$ and $\sigma$, but this corresponds to the mean and standard deviation of the normally distributed logarithm $log(Y)$.  Thus $\mu$ and $\sigma$ are on a different scale than the variable that is log-normally distributed. 

This also means that you can create a log-normal distribution by exponentiating the samples of a normal distribution:



```{r lognormal, fig.height=2,fig.width=3.5, fig.show='hold', message=F, warning=F}
mu <- 6
sigma <- 0.5
N <- 100000
# We generate N random samples from a log-normal distribution.
sl <- rlnorm(N, mu, sigma)
lognormal_plot <- ggplot(data.frame(samples=sl), aes(sl)) + geom_histogram() + 
      ggtitle("Log-normal distribution\n") + ylim(0,25000) + xlim(0,2000)
# We generate N random samples from a normal distribution, and then we exponentiate them
sn <- exp(rnorm(N, mu, sigma))
normalplot <- ggplot(data.frame(samples=sn), aes(sn)) + geom_histogram() + 
      ggtitle("Exponentiated samples of\na normal distribution") + ylim(0,25000) + xlim(0,2000)

plot(lognormal_plot)
plot(normalplot)
```



### The slightly more realistic probability model

If we assume that RTs are log-normally distributed, we'll need to change our model:

\begin{equation}
Y_i \sim LogNormal(\alpha + presses_i \cdot \beta,\sigma)
\end{equation}

where $i =1 \ldots N$


But now the scale of our priors needs to change! They are no longer in milliseconds.

\begin{equation}
\begin{aligned}
\alpha &\sim Normal(0, 10) \\
\beta &\sim Normal(0, 1) \\
\sigma &\sim Normal(0, 2) \text{ truncated so that } \sigma > 0 \\
\end{aligned}
\end{equation}

The interpretation of the parameters changes and it is more
complex than if we were dealing with a linear model (that is, with  a normal distribution):

* $\alpha$. In our previous linear model, $\alpha$ represented the grand mean (or the grand median since in a normal distribution both coincide), and was equivalent to our previous $\mu$ (since $\beta$ was multiplied by 0). But now, the grand mean needs to be calculated in the following way,  $\exp(\alpha +\sigma ^{2}/2)$. Interestingly, the grand median will  just be $exp(\alpha)$,^[You can check in Wikipedia (https://en.wikipedia.org/wiki/Log-normal_distribution) why.] and we could assume that this represents the underlying time it takes to press the space bar if there would be no noise, that is, if $\sigma$ had no effect. This also means that the prior of $\alpha$ is not in milliseconds, but in log(milliseconds).

*  $\beta$. In a linear model, $\beta$ represents the slowdown for each time the space bar is pressed. Now $\beta$ is the effect on the log-scale, and the effect in milliseconds depends on the intercept $\alpha$: $exp(\alpha + \beta) - exp(\alpha)$. Notice that the log is not linear and the effect of  $\beta$ will have more impact on milliseconds as the intercept grows. For example, if we start with (i) $exp(5) = 148$, and we add $0.1$ in log-scale, $exp(5 + 0.1) = 164$, we end up with a difference of 15 ms; if we start with (ii) $exp(6) = 400$, and we add $0.1$, $exp(6 + 0.1) = 445$, we end up with a difference of 45 ms. You can also see it graphically below.

*  $\sigma$. This is the standard deviation of the normal distribution of $log(y)$.

<!-- ($exp(10+.1) - exp(10) >  exp(1+.1) - exp(1)$, that is, $`r exp(10+.1) - exp(10)` > `r exp(1+.1) - exp(1)`$). -->

```{r, echo=F}

ms_diff <- function(Intercept){
  exp(Intercept + .1) - exp(Intercept)
}
df <- tibble::data_frame(Intercept=seq(.1,15,.01), ms= ms_diff(Intercept))
ggplot(df, aes(x=Intercept,y=ms)) + geom_point() + scale_y_continuous("Difference in milliseconds")

```

What kind of information are the priors encoding?


* For $\alpha$: We are 95% certain that the grand median of the RTs will be between $\approx `r exp(-10*2)`$ and $`r round(exp(10*2))`$ milliseconds. This is a (very-)weakly regularizing prior  because it won't affect our results, but it will down-weight values for the grand median of the RTs that are extremely large, and won't allow the grand median to be negative. We calculate the previous range by back-transforming the values that lie between two standard deviations of the prior ($2 \times 10$) to millisecond scale: $exp(-10 \times 2)$ and $exp(10 \times 2)$).

* For $\beta$: This is more complicated, because the effect on milliseconds will depend on the estimate of $\alpha$. However, we can assume some value for $\alpha$ and it will be enough to be in the right order of magnitude. So let's assume 500 ms. That will mean that we are 95% certain that the effect of pressing the space bar will be between $`r round(exp(log(500) - 4)) - 500`$ and $`r round(exp(log(500) + 4)) - 500`$ milliseconds. It is asymmetric because the log-scale is asymmetric. But the prior is weak enough so that if we assume 1000 or 100 instead of 500, the possible estimates of $\beta$ will still be contained in the prior distribution.  We calculate this by first finding out the value in milliseconds when we are two standard deviations away in both directions: ($2\times 2$), that is $exp(500 - 2 \times 2)$ and  $exp(500 + 2 \times 2)$, and we subtract from that the value of $\alpha$ that we assumed, $500$:   $exp(500 - 2 \times 2) - 500$ and  $exp(500 + 2 \times 2) - 500$.

*  For $\sigma$. This indicates that we are 95% certain that the standard deviation of $log(y)$ will be between 0 and 2. So 95% of the RTs  will be between $exp(log(500) - 1 \times 2) = `r round(exp(log(500) - 1 * 2))`$ and $exp(log(500) + 1 \times 2) = `r round(exp(log(500) + 1 * 2))`$.

What happens if we replace 500 by 100, and by 1000? What happens if it is 10 instead? Does it still makes sense?



We'll code the model as follows. 

```{r}
priors_log <- c(set_prior("normal(0, 10)", 
                      class = "Intercept"),
            set_prior("normal(0, 1)", 
                      class = "b",
                      coef="presses"),
            set_prior("normal(0, 2)", 
                      class = "sigma"))
```

```{r cache=TRUE}
m_logn<-brm(RT~1+presses,noreading_data,prior = priors_log,
       iter = 2000, chains = 4,family = lognormal(), 
       control = list(adapt_delta = 0.99,max_treedepth=15))
summary(m_logn)
```

We fit the model, and check its convergence as usual.



### Summarizing the posterior and inference

```{r, echo=F, results="hide"}
options(scipen=999, digits=6)

alpha_samples<-posterior_samples(m_logn,"^b")$b_Intercept

beta_samples<-posterior_samples(m_logn,"^b")$b_presses

beta_ms<-exp(alpha_samples+beta_samples)-exp(alpha_samples)

beta_msmean <- round(mean(beta_ms),5)
beta_mslow <- round(quantile(beta_ms,prob=0.025),5)
beta_mshigh <- round(quantile(beta_ms,prob=0.975),5)

beta_mean <- round(mean(beta_samples),5)
beta_low <- round(quantile(beta_samples,prob=0.025),5)
beta_high <- round(quantile(beta_samples,prob=0.975),5)
```


We can summarize the posterior and do inference as before. If we want to talk about the effect estimated by the model, we summarize the posterior of $\beta$ in the following way: $\hat\beta = `r mean(beta_samples)`$, 95% CrI = $[ `r beta_low` , `r beta_high` ]$, $P(\beta >0)  \approx `r mean(beta_samples > 0)`$




But sometimes, the effect in milliseconds is easier to interpret. We generated the 
effect of 1 press in the generated quantities block, which is not the same as
the linear model's $\beta$. Our generated estimate will tell us the estimate
of the slowdown produced by pressing the space bar in the middle of the
experiment once, assuming that the RTs are log-normally distributed:
$`r beta_msmean`$ ms, 95% CrI = $[ `r beta_mslow` , `r beta_mshigh` ]$. 
Coincidentally, it is close to the
same value as before, but this is not always the case, and since it's not
linear the effect won't be the same across the whole experiment; see Exercise
\ref{ex:diff-eff}.

### Posterior predictive checks and distribution of summary statistics 

We can now verify whether our predicted datasets look more similar to the real dataset.

```{r, message=FALSE, warning=FALSE, fig.height=4}
pp_check(m_logn, nsamples = 100)+
  theme(text = element_text(size=16),legend.text=element_text(size=16))
```

*Is the simulated data now more similar to the real data?*

It seems so, but it's not easy to tell. Another way to examine this would be
to look at the  *distribution of summary statistics*. The idea is to compare the
distribution of representative summary statistics for the datasets generated
by different models and compare them to the observed statistics. Since we
suspect that the log-normal distribution may capture the long tail,  we could
use the maximum as a summary statistics.

to-do

Here we see that both distributions are unable to capture the maximum value of the observed data, and that there's still room for improving the model. Another more advanced possibility is to compare the models using cross-validation techniques; see for example, @GelmanEtAl2014understanding; @VehtariOjanen2012; @VehtariEtAl2017.



### General workflow

This is the general workflow that we recommend for a Bayesian model.

1. Define the full probability model:
    a. Decide on the likelihood.
    b. Decide on the priors.
    c. Write the brms model.
2. Check model using fake data simulations:
    a. Simulate data with known values for the parameters.
    b. Fit the model and do MCMC diagnostics.
    c. Verify that it recovers the parameters from simulated data.
3. Fit the model with real data and do MCMC diagnostics.
4. Evaluate the model's fit (e.g., posterior predictive checks, distribution of summary statistics). This may send you back to 1.
5. Inference/prediction/decisions.
6. Sometimes model comparison if there's an alternative model (to be discussed later).





\newpage

\begin{Sbox}{\subsection{Key concepts}}
\begin{itemize}
\item Basic brms syntax.
\item How to identify convergence problems (MCMC diagnostics).
\item Normal and log-normal distributions.
\item Summaries of the posterior and inference.
\item Model evaluation using posterior predictive checks.
\item General workflow for doing Bayesian analysis.
\end{itemize}
\end{Sbox}

\begin{qbox}{\subsection{Exercises}}
\begin{enumerate}
\item For the  model  in Section \ref{sec:first}.
  \begin{enumerate}
    \item Recall that when we use the Beta distribution as a prior for the $\theta$ parameter in a Binomial, the two parameters of the Beta indicate previous outcomes and not only the most likely value of $\theta$. For example, $Beta(2,2)$ and $Beta(50,50)$ both indicate that $.5$ is the most likely value of $\theta$, but $Beta(50,50)$ indicates that we are more certain about it than $Beta(2,2)$. 
    Assume that you are quite certain that the average accuracy in the task is 80\% for individuals with aphasia. How would you change the priors for the model in \ref{sec:first}? Fit the model several times, assuming the same average accuracy as prior information, but varying the amount of uncertainty? When do the results change? \label{ex:firstpriors}
    \item Change the data to \verb|qresp_data <- list(c = 460, N = 1000)|, and repeat \ref{ex:firstpriors} with the same priors.
    \item Simulate data assuming that the true $\theta$ is exactly $.5$ and (i) \verb|N = 100|, (ii) \verb|N = 1000| and run the \verb|firstmodel.stan|. (Tip: use the function \verb|rbinom| to simulate data.) Is the true value of $\theta$ inside 95\% credible interval? \label{ex:first-fake}
 \end{enumerate}
 \item For the  model  in Section \ref{sec:small}
  \begin{enumerate}
  \item Change the priors of $\mu$ and $\sigma$ in \verb|noreading_1.stan| to a Cauchy and a truncated Cauchy distributions. A Cauchy distribution is basically a bell shaped distribution with very fat tails.\footnote{You can read about this distribution in \url{https://en.wikipedia.org/wiki/Cauchy_distribution}.} What are reasonable values for the location and scale? What's the difference between having a Cauchy or a normal distribution as priors?
  \end{enumerate}
\end{enumerate}
Continues in the next page.
\end{qbox}

\begin{qbox}{}
\begin{enumerate}[3.]
\item For the  models  in Section \ref{sec:moreinter}.
  \begin{enumerate}
  \item Edit the \verb|generated quantities| block in \verb|noreading_log.stan| to estimate the slowdown in milliseconds (mean and 90\% credible interval) for the last time the participant  pressed the space bar in the experiment. In addition, predict the slowdown if the experiment would have had 500 observations.\label{ex:diff-eff}
  \item Optional but very recommended: Simulate data that assumes a value of $\alpha$ of 400 and $\sigma$ of 125 and a practice effect of (i) 0.00001  and (ii) 0.1 (both in log-scale). Fit these data with \verb|noreading_log.stan|. Is the true value of the effect in the 95\% credible interval in both cases? 
  \item Add word length as a covariate in \verb|noreading_log.stan|, summarize the posterior. How does the length of the words affect RTs?
   \end{enumerate}
\end{enumerate}
\end{qbox}



\newpage

## Appendix - Troubleshooting problems of convergence \label{sec:Appendix}


1. Rhat > 1.1 First of all check that there are no silly mistakes in the model. Forgetting to put parenthesis, multiplying the wrong parameters, using the wrong operation, etc. can create a model that can't converge. As our models grow in complexity there are more places where to make mistakes. Start simple, see if the model works, add complexity slowly, checking if the model converges at every step. In very rare occasions, when Rhat >1.1 and the model is correct, it may help to increase the number of iterations, but then it's usually a better idea to re-parametrize the model, see 3.

2. Stan gives a warning. The solution may also be point 1. But if the model is correctly specified, you should check  Stan's website, there is a very good guide to solve problems in: http://mc-stan.org/misc/warnings.html. If this doesn't work, you may need to re-parametrize the model, see 3.

3. Some models have convergence issues because the sampler struggles to explore the parameters space. This is specially relevant in complex hierarchical models. In this case, the solution might be to re-parametrize the model. This is by no means trivial. However, the simplest parametrization trick to try is to have all the priors on the same rough scale, that is priors shouldn't have different orders of magnitude. You can find some suggestions in the chapter 21 of Stan manual [@Stan2017], and the following case study: http://mc-stan.org/users/documentation/case-studies/qr_regression.html.


<!-- https://cran.r-project.org/web/packages/bayesplot/vignettes/PPC.html -->

