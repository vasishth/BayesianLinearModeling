# Hierarchical linear modeling 

## Example 1: Reading time differences in subject vs object relatives in English

We begin with a classic question from the psycholinguistics literature: are subject relatives easier to process than object relatives? The data come from Experiment 1 in a paper by @grodner.

### Scientific question: Is there a subject relative advantage in reading?

In two important papers, @gibson00 and @grodner suggest that object relative clause sentences are more difficult to process than subject relative clause sentences because the distance between the relative clause verb *sent* and the head noun phrase of the relative clause, *reporter*, is longer in object vs subject relatives. Examples are shown below.

(1a) The *reporter* who the photographer *sent* to the editor was hoping for a good story. (object gap)

(1b) The *reporter* who *sent* the photographer to the editor was hoping for a good story. (subject gap)

The underlying explanation has to do with memory processes: shorter linguistic dependencies are easier to process due to either reduced interference or decay, or both. For implemented computational models that spell this point out, see @lewisvasishth:cogsci05 and @EngelmannJaegerVasishthSubmitted2018.

In the Grodner and Gibson data, the dependent measure is reading time at the relative clause verb, in milliseconds. We are expecting longer reading times in object gap sentences compared to subject gap.

### Load data and reformat

```{r open_swetsetal}
gg05e1 <- read.table("data/GrodnerGibson2005E1.csv",sep=",", header=T)
gge1 <- gg05e1 %>% filter(item != 0)

gge1 <- gge1 %>% mutate(word_positionnew = ifelse(item != 15 & word_position > 10, word_position-1, word_position)) 
#there is a mistake in the coding of word position,
#all items but 15 have regions 10 and higher coded
#as words 11 and higher

## get data from relative clause verb:
gge1crit <- subset(gge1, ( condition == "objgap" & word_position == 6 ) |
            ( condition == "subjgap" & word_position == 4 ))
```

### Experiment design: Latin square and crossed subject and items

Two important properties of these data are worth noticing. 

#### Latin-square design

First, the design is the classic repeated measure Latin square set-up. To see what this means, 
first look at the number of subjects and items, and the number of rows in the data frame:

```{r}
length(unique(gge1crit$subject))
length(unique(gge1crit$item))
dim(gge1crit)[1]
```

There are 42 subjects and 16 items. There are $42\times 16 = 672$ rows in the data frame. Notice also that each subject sees exactly eight object gap and eight subject gap sentences:

```{r}
head(xtabs(~subject+condition,gge1crit),n=4)
```

The researchers created 16 sets of subject and object relatives; one set is the pair of sentences shown in (1a) and (1b) above. In the data frame, both these two items have the same id 1, but no one subject will see both sentences in any one set. For example, item 1 is seen by subject 1 in the object gap condition (1a) and item 1 is seen by subject 2 in the subject gap condition (1b):

```{r}
subset(gge1crit,item==1)[1:2,]
```


\begin{table}[!htbp]
\caption{The Latin-square design in repeated measures experiments.}
\begin{center}
\begin{tabular}{ccc}
item id & group 1 & group 2\\
1       & objgap       & subjgap \\
2       & subjgap       & objgap \\
3       & objgap       & subjgap \\
4       & subjgap       & objgap \\
\vdots  & \vdots  & \vdots \\
16      & subjgap       & objgap \\
\end{tabular}
\end{center}
\label{tab:latinsq}
\end{table}

This is called a Latin-square design because of the following layout. See Table \ref{tab:latinsq}.
Each subject is randomly assigned to Group 1 or 2, and one should have an even number of subjects in order to have a balanced data-set. Hence the 42 subjects in the Grodner and Gibson data: 21 in each group.

A useful way to ensure that you have balanced assignments of subjects to each group is to randomize the order of incoming participants in advance, such that pairs of subjects are assigned to group 1 and 2. Let order1 be such that the first subject gets group 1 and the second gets group 2, and order 2 that the first subject gets group 2 and the second group 1. Then just generate a random ordering to ensure that each pair of subjects lands in a balanced way across groups:

```{r}
sample(rep(c("order1","order2"),11))
```

Latin square designs are used in psychology and linguistics (and other areas) because they are optimal in several ways.

Soon we will need to generate a fake data-frame with a repeated measures Latin square design. We can do this using R as follows [source: @VasishthMertzenJaegerGelman2018]: 

```{r}
library(MASS)
nitem <- 16
nsubj <- 42
## prepare data frame for two condition in a latin square design:
g1<-data.frame(item=1:nitem,
                 cond=rep(c("objgap","subjgap"),nitem/2))
g2<-data.frame(item=1:nitem,
                 cond=rep(c("objgap","subjgap"),nitem/2))

## assemble data frame in long format:
gp1<-g1[rep(seq_len(nrow(g1)),
              nsubj/2),]
gp2<-g2[rep(seq_len(nrow(g2)),
              nsubj/2),]

fakedat<-rbind(gp1,gp2)
## sanity check:
dim(fakedat)
## add subjects:
fakedat$subj<-rep(1:nsubj,each=nitem)
fakedat<-fakedat[,c(3,1,2)]  
fakedat$so<-ifelse(fakedat$cond=="objgap",1,-1)
```

For example, subject 1 sees the following conditions and items:

```{r}
head(fakedat,n=16)
```

We will need this code later for fake data simulation.

#### Fully crossed subjects and items

In the data, because of the Latin square design, each subject sees exactly one item in one of the two conditions:

```{r}
xtabs(~subject+item,gge1crit)
```

If there were some zeros in the above matrix, we would have an imbalance, and this would then be  *partially crossed*. This kind of imbalance arises in data-sets due to missing data, where missingness can happen due to different reasons. E.g., in eyetracking, subjects sometimes skip the critical word entirely, or there is tracking loss; these events lead to a 0 ms reading time being recorded, and this could be treated as missing data (marked as NA). We return to this point in an advanced course on Bayesian modeling, to be offered at some later date.

### The implied generative model

The above design implies a particular statistical model that takes us beyond the linear model.

To remind you, a simple linear model of the above data would be:

\begin{equation}
y = \alpha + \beta * x + \varepsilon \hbox{ where } \varepsilon \sim Normal(0,\sigma)
\end{equation}

Here, object gaps are coded +1, subject gaps -1. See @SchadEtAlcontrasts for an explanation of contrast coding.

```{r}
gge1crit$so<-ifelse(gge1crit$condition=="objgap",1,-1)
```

As figure \ref{fig:ggrtdistrn} shows, a Normal likelihood doesn't seem well motivated, so we will use the log-normal.

```{r fig.cap="\\label{fig:ggrtdistrn}Distribution of reading times in the Grodner and Gibson Experiment 1 data, at the critical region."}
plot(density(gge1crit$rawRT),main="Grodner and Gibson Expt 1",xlab="RTs (ms)")
```

#### Between subject variability in mean reading time

The simple linear model above would ignore the fact that we have 
repeated measures from multiple subjects---the iid assumption is violated.
Also, different subjects that may have different mean reading times ($\alpha$ differ for each subject) and different object gap processing costs ($\beta$ differs for each subject). Some subjects will be slower and some faster, and some may suffer more with object gaps because of lower working memory capacity, lower attention, etc. (of course, we are speculating here, we have no measurements of these individual difference variables).  See Figure \ref{fig:subjint} for a visualization of between subject variability in mean reading times.

```{r fig.cap="\\label{fig:subjint}Between subject variability in mean reading times."}
hist(with(gge1crit,tapply(rawRT,subject,mean)),
     main="Between subject variability",
     xlab="mean RTs",freq=FALSE)
```

In the linear model, we can express the assumption that the grand mean intercept $\alpha$ needs an adjustment by subject, where subjects are indexed from $j=1,\dots,J$:

\begin{equation}
y_j = \alpha + u_{0j} + \beta * x_j + \varepsilon_j 
\end{equation}

where we now have two sources of variance:

- within subject variance, $\varepsilon_j \sim Normal(0,\sigma)$ 
- between subject variance in mean reading times, $u_{0j} \sim Normal(0,\sigma_{u0})$ 

In Bayes, the adjustment $u_{0j}$ is a parameter. This is not the case in the frequentist paradigm [@lme4new].

#### Between item variability in mean reading time

We also see that items also differ, some would be read faster and some slower:

```{r fig.cap="\\label{fig:itemint}Between item variability in mean reading times."}
hist(with(gge1crit,tapply(rawRT,item,mean)),
     main="Between item variability",
     xlab="mean RTs",freq=FALSE)
```


For items ranging from $k=1,\dots,K$, we can add this assumption to the model:

\begin{equation}
y_{kj} = \alpha + u_{0j} + w_{0k} + \beta * x_{kj} + \varepsilon_{kj}
\end{equation}

where there are now three variance components:

- $\varepsilon_{kj} \sim Normal(0,\sigma)$
- $u_{0j} \sim Normal(0,\sigma_{u0})$
- between item variability in mean reading time, $w_{0k} \sim Normal(0,\sigma_{w0})$

This model is called a *varying intercepts model* with crossed varying intercepts for subjects and for items.

#### Between subject and between item variability in objgap cost

The object gap cost can also vary by subject and by item. See Figure \ref{fig:subjitemslope}. 

```{r fig.cap="\\label{fig:subjitemslope}Between subject and item variability in object gap vs subject gap reading times."}
op<-par(mfrow=c(1,2),pty="s")
meanssubj<-with(gge1crit,
                tapply(rawRT,IND=list(subject,condition),mean))
diff<-meanssubj[,2]-meanssubj[,1]

hist(diff,
     main="Between subject variability",xlab="mean objgap cost",freq=FALSE)

meansitem<-with(gge1crit,tapply(rawRT,IND=list(item,condition),mean))
diff<-meansitem[,2]-meansitem[,1]

hist(diff,
     main="Between item variability",xlab="mean objgap cost",freq=FALSE)
```

We can incorporate this assumption into the model by adding adjustments to the $\beta$ parameter:

\begin{equation}
y_{kj} = \alpha + u_{0j} + w_{0k} + (\beta + u_{1j} + w_{1k}) * x_{kj} + \varepsilon_{kj}
\end{equation}

where 

  - $\varepsilon_{kj} \sim Normal(0,\sigma)$ 
  - $u_{0j} \sim Normal(0,\sigma_{u0})$
  - $u_{1j} \sim Normal(0,\sigma_{u1})$
  - $w_{0k} \sim Normal(0,\sigma_{w0})$
  - $w_{1k} \sim Normal(0,\sigma_{w1})$

This is called the *varying intercepts and slopes* model with *no correlation* between the intercepts and slopes.

There is one detail still missing in the model: the adjustments to the intercept and slope are correlated for subjects, and also for items. In other words, we have a bivariate distribution for the subject and item random effects:

\begin{equation}
y_{kj} = \alpha + u_{0j} + w_{0k} + (\beta + u_{1j} + w_{1k}) * x_{kj} + \varepsilon_{kj}
\end{equation}

where $\varepsilon_{kj} \sim Normal(0,\sigma)$ and 

\begin{equation}\label{eq:covmat}
\Sigma _u
=
\begin{pmatrix}
\sigma _{u0}^2  & \rho _{u}\sigma _{u0}\sigma _{u1}\\
\rho _{u}\sigma _{u0}\sigma _{u1}    & \sigma _{u1}^2\\
\end{pmatrix}
\quad 
\Sigma _w
=
\begin{pmatrix}
\sigma _{w0}^2  & \rho _{w}\sigma _{w0}\sigma _{w1}\\
\rho _{w}\sigma _{w0}\sigma _{w1}    & \sigma _{w1}^2\\
\end{pmatrix}
\end{equation}

\begin{equation}\label{eq:jointpriordist1}
\begin{pmatrix}
  u_0 \\ 
  u_1 \\
\end{pmatrix}
\sim 
\mathcal{N} \left(
\begin{pmatrix}
  0 \\
  0 \\
\end{pmatrix},
\Sigma_{u}
\right),
\quad
\begin{pmatrix}
  w_0 \\ 
  w_1 \\
\end{pmatrix}
\sim 
\mathcal{N}\left(
\begin{pmatrix}
  0 \\
  0 \\
\end{pmatrix},
\Sigma_{w}
\right)
\end{equation}

This is a varying intercepts and slopes model with fully specified  variance-covariance matrices for the subject and item random effects. It is sometimes called the **maximal model**.

### Implementing the model

The above model is simple to implement in the Bayesian framework.

#### Specify and visualize priors

We define some priors first:

\begin{enumerate}
\item $\alpha \sim Normal(0,10)$
\item $\beta \sim Normal(0,1)$
\item Residual standard deviation: $\sigma \sim Normal(0,1)$
\item All other standard deviations: $\sigma \sim Normal(0,1)$
\item Correlation matrix: $\rho \sim LKJ(2)$. 
\end{enumerate}

The LKJ prior needs some explanation.

#### The LKJ prior on the correlation matrix

In this model, we assume that the vector 
$\mathbf{u}=\langle u_0, u_1 \rangle$
comes from a bivariate normal distribution with a variance-covariance
matrix $\boldsymbol{\Sigma_u}$. This matrix has the variances of the adjustment to the intercept and to the
slope respectively along the diagonal, and the covariance on the off-diagonals. 

Recall that the covariance $Cov(X,Y)$ between two variables $X$ and $Y$ is
defined as the product of their correlation $\rho$ and their standard
deviations $\sigma_X$ and $\sigma_Y$, such that, $Cov(X,Y) = \rho
\sigma_X \sigma_Y$.

\begin{equation}
\boldsymbol{\Sigma_u} = 
{\begin{pmatrix} 
\sigma_{u_0}^2 & \rho_u \sigma_{u_0} \sigma_{u_1} \\ 
\rho_u \sigma_{u_0} \sigma_{u_1} & \sigma_{u_1}^2
\end{pmatrix}}
\end{equation}

The covariance matrix can be decomposed into a vector of standard deviations and a correlation matrix. The correlation matrix looks like this:

\begin{equation}
{\begin{pmatrix} 
1 & \rho_u  \\ 
\rho_u  & 1
\end{pmatrix}}
\end{equation}

In Stan, we write a matrix that has 0's on the off-diagonals as:

\begin{equation}
diag\_matrix(\sigma_{u_0},\sigma_{u_1}) = 
\begin{pmatrix} 
\sigma_{u_0} & 0 \\ 
0  & \sigma_{u_1}
\end{pmatrix}
\end{equation}

This means that we can decompose the covariance matrix into three parts:

\begin{equation}
\begin{aligned}
\boldsymbol{\Sigma_u} &= diag\_matrix(\sigma_{u_0},\sigma_{u_1}) \cdot \boldsymbol{\rho_u} \cdot diag\_matrix(\sigma_{u_0},\sigma_{u_1})\\
&=
{\begin{pmatrix} 
\sigma_{u_0} & 0 \\ 
0  & \sigma_{u_1}
\end{pmatrix}}
{\begin{pmatrix} 
1 & \rho_u  \\ 
\rho_u  & 1
\end{pmatrix}}
{\begin{pmatrix} 
\sigma_{u_0} & 0 \\ 
0  & \sigma_{u_1}
\end{pmatrix}}
\end{aligned}
\end{equation}

So we need priors for the $\sigma_u$s and for $\rho_u$:

The basic idea of the  LKJ prior is that its parameter
(usually called *eta*, $\eta$, here is $2$) increases, the prior increasingly concentrates around
the unit correlation matrix (i.e., favors smaller correlation: ones in the
diagonals and values close to zero in the lower and upper triangles). At $\eta
= 1$, the LKJ correlation distribution is uninformative (similar to
$Beta(1,1)$), at $\eta < 1$, it favors extreme correlations  (similar to
$Beta(a<1,b<1)$).


#### Visualize the priors

As always, it is a good idea to visualize these priors. See Figure \ref{fig:priorsgg}.

```{r results="hide"}
priors_alpha <- c(0,10)
priors_beta <- c(0,1)
priors_sigma_e <- c(0,1)
priors_sigma_u <- c(0,1)
priors_sigma_w <- c(0,1)

## code for visualizing lkj priors:
fake_data <- list(x = rnorm(30,0,1),
                  N = 30, R = 2) 

stancode <- "
data {
  int<lower=0> N; 
  real x[N]; 
  int R;
  }
parameters {
  real mu;
  real<lower=0> sigma;
}
model {
  x ~ normal(mu,sigma);  
}
generated quantities {
  corr_matrix[R] LKJ05;
  corr_matrix[R] LKJ1;
  corr_matrix[R] LKJ2;
  corr_matrix[R] LKJ4;
  LKJ05 = lkj_corr_rng(R,.5);
  LKJ1 = lkj_corr_rng(R,1);
  LKJ2 = lkj_corr_rng(R,2);
  LKJ4 = lkj_corr_rng(R,4);
}
"

fitfake <- stan(model_code = stancode, pars = c("LKJ05","LKJ1","LKJ2","LKJ4"),
                data = fake_data, chains = 4, 
                iter = 2000)

corrs<-extract(fitfake,pars=c("LKJ05[1,2]","LKJ1[1,2]","LKJ2[1,2]","LKJ4[1,2]"))
```

```{r visualizepriors, fig.cap="\\label{fig:priorsgg}Priors for the Godner and Gibson data."}
op<-par(mfrow=c(2,3),pty="s")
par(oma = rep(0, 4), mar = c(2.7, 2.7, 0.1, 0.1), mgp = c(1.7, 0.4, 0))
b<-seq(-priors_alpha[2]*2,priors_alpha[2]*2,by=0.01)
plot(b,dnorm(b,mean=priors_beta[1],sd=priors_beta[2]),type="l",ylab="density", 
     xlab=expression(alpha),ylim=c(0, 0.5))
plot(b,dnorm(b,mean=priors_beta[1],sd=priors_beta[2]),type="l",ylab="density",
     xlab=expression(beta),ylim=c(0, 0.5))
sig<-seq(0,priors_sigma_e[2]*3,by=0.01)
plot(sig,dnorm(sig,mean=priors_sigma_e[1],sd=priors_sigma_e[2]),type="l",ylab="density",
     xlab=expression(sigma[e]))
plot(sig,dnorm(sig,mean=priors_sigma_u[1],sd=priors_sigma_u[2]),type="l",ylab="density",
     xlab=expression(sigma[u[0]]))
plot(sig,dnorm(sig,mean=priors_sigma_u[1],sd=priors_sigma_u[2]),type="l",ylab="density",
     xlab=expression(sigma[w[0,1]]))
plot(density(corrs[[3]],bw=0.15),ylab="density",xlab=expression(rho),xlim=c(-1,1),main="")
```

We will return later to the question of whether these priors are appropriate and reasonable (short answer: no).

### Fit the model using brms

```{r results="hide"}
priors <- c(set_prior("normal(0, 10)", class = "Intercept"),
                      set_prior("normal(0, 1)", class = "b", 
                                coef = "so"),
                      set_prior("normal(0, 1)", class = "sd"),
                      set_prior("normal(0, 1)", class = "sigma"),
                      set_prior("lkj(2)", class = "cor"))

m_gg<-brm(rawRT~so + (1+so|subject) + (1+so|item),gge1crit,family=lognormal(),
    prior=priors)
```

```{r}
summary(m_gg)
```

```{r fig.cap="\\label{fig:ggpost}Posterior distributions of parameters in the Grodner and Gibson data."}
stanplot(m_gg,type="hist")
```

Figure \ref{fig:ggpost} shows the posterior distributions of the parameters on the log ms scale (for the coefficients and standard deviations). Notice that

\begin{itemize}
\item The object relative takes longer to read than the subject relative, as predicted. We know this because the parameter b\_so is positive.
\item The largest sources of variance are the subject intercepts, slopes, and the residual standard deviation. Look at the sd\_subject parameters, and sigma.
\item The by-item variance components are relatively small. Look at the sd\_item parameters.
\item The correlations have very wide uncertainty---the prior is dominating in determining the posteriors as there isn't that much data to obtain accurate estimates of these parameters. Look at the cor parameters.
\end{itemize}

### Examine by subject random effects visually

First, extract the posterior samples of the parameters that we will need to compute individual differences.

```{r}
library(bayesplot)
postgg<-posterior_samples(m_gg)
## extract variances:
alpha<-postgg$b_Intercept
beta<-postgg$b_so
cor<-posterior_samples(m_gg,"^cor")
sd<-posterior_samples(m_gg,"^sd")
sigma<-posterior_samples(m_gg,"sigma")

## item random effects won't be used below
item_re<-posterior_samples(m_gg,"^r_item")
subj_re<-posterior_samples(m_gg,"^r_subj")
```


#### By subject intercept adjustments

Figure \ref{fig:ggsubjint} shows the adjustments to the intercept ($\alpha$) by subject. This is on the log scale. Here, we are looking at the parameters $u_0$ in the model, which are assumed to be generated from $Normal(0,\sigma_{u0})$. The between subject variability is being captured here by this subject level parameter.

```{r fig.cap="\\label{fig:ggsubjint}Variability in subject intercept adjustments in the Grodner and Gibson data."}
subjint<-subj_re[,1:42]
colnames(subjint)<-c(paste("u0,",1:42,sep=""))
intmns <- colMeans(subjint)
subjint<-subjint[,order(intmns)]
mcmc_areas(subjint)
```

#### By subject slope adjustments

Figure \ref{fig:ggsubjslope} shows the by-subject adjustments to the OR processing cost effect ($\beta$). Here, the assumption is that these adjustments are $u_1 \sim Normal(0,\sigma_{u_1})$.

```{r fig.cap="\\label{fig:ggsubjslope}Variability in subject slope adjustments in the Grodner and Gibson data."}
subjslope<-subj_re[,(1:42)+42]
colnames(subjslope)<-c(paste("u1,",1:42,sep=""))
slopemns <- colMeans(subjslope)
subjslope<-subjslope[,order(slopemns)]
mcmc_areas(subjslope)
```

The correlation between $u_0$ and $u_1$ is represented by the correlation parameter $\rho_u$.

```{r fig.cap="\\label{fig:ggpostcorr}Posterior distributions of subject varying intercept and slope correlation parameter in the Grodner and Gibson data."}
stanplot(m_gg,type="hist",pars="cor_subject__Intercept__so")
```


### Examine mean and individual differences on the raw ms scale

It is useful to see the effects on the raw ms scale. The log ms scale is difficult to interpret. 


#### Mean difference

In psychology and linguistics, undue emphasis is paid on reporting the overall mean effect. Figure \ref{fig:ggmeandiff} shows this---there seems to be a clear 50 ms OR processing cost effect, with 95% credible interval 

```{r}
meandiff<- exp(alpha + beta) - exp(alpha - beta)
mean(meandiff)
round(quantile(meandiff,prob=c(0.025,0.975)),0)
```

```{r fig.cap="\\label{fig:ggmeandiff}Mean OR processing cost effect in the Grodner and Gibson data."}
hist(meandiff,freq=FALSE,main="Mean OR vs SR processing cost",xlab=expression(exp(alpha + beta)- exp(alpha - beta)))
```

#### Individual effects of OR processing cost

However, only three out of 42 subjects show clear evidence for OR processing cost. 
See Figure \ref{fig:ggsubjeffect}.

```{r fig.cap="\\label{fig:ggsubjeffect}Variability in subject OR processing cost effect in the Grodner and Gibson data."}
subjdiff<-matrix(rep(NA,42*4000),nrow=42)
for(i in 1:42){
subjdiff[i,]<-exp(alpha + subj_re[,i]  + (beta+subj_re[,i+42])) - exp(alpha + subj_re[,i] - (beta+subj_re[,i+42]))
}

subjdiff<-t(subjdiff)

subjdiff<-as.data.frame(subjdiff)
colnames(subjdiff)<-c(1:42)
mns <- colMeans(subjdiff)
subjdiff<-subjdiff[,order(mns)]
mcmc_areas(subjdiff)
```

**What is the relevance of a mean effect that doesn't even represent the behavior of the majority or even a reasonable proportion of participants even in the sample we have?** The answer is: not much! Most of the interesting action is going down in the individual level effects.

As Spiegelhalter et al write in the Norm Chronicles: ``The average is an abstraction. The reality is variation.'' Any theory of sentence processing would have to be able to reproduce the pattern of individual differences observed, with only a few participants showing an OR processing cost and the majority showing equivocal conclusions.

### To make discovery claims, calibrate the true and false discovery rate

Suppose that, based on these data and this model, we want to claim that there is a mean OR processing cost in English. In order to make a discovery claim, we need to understand the **true discovery rate** of this effect. In the frequentist world, this would be the *statistical power*, the probability of detecting an effect if there is in fact one.

First, we write a function to generate fake data:

```{r}
library(MASS)
gen_fake_lnorm <- function(nitem=16,nsubj=42,
alpha=NULL,beta=NULL,
                        Sigma_u=NULL,Sigma_w=NULL,sigma_e=NULL){
  ## prepare data frame for two condition in a latin square design:
g1<-data.frame(item=1:nitem,
                 cond=rep(c("objgap","subjgap"),nitem/2))
g2<-data.frame(item=1:nitem,
                 cond=rep(c("objgap","subjgap"),nitem/2))

## assemble data frame in long format:
gp1<-g1[rep(seq_len(nrow(g1)),
              nsubj/2),]
gp2<-g2[rep(seq_len(nrow(g2)),
              nsubj/2),]

fakedat<-rbind(gp1,gp2)

## add subjects:
fakedat$subj<-rep(1:nsubj,each=nitem)
fakedat<-fakedat[,c(3,1,2)]  
fakedat$so<-ifelse(fakedat$cond=="objgap",1,-1)

## subject random effects:
  u<-mvrnorm(n=length(unique(fakedat$subj)),
             mu=c(0,0),Sigma=Sigma_u)
  ## item random effects
  w<-mvrnorm(n=length(unique(fakedat$item)),
             mu=c(0,0),Sigma=Sigma_w)
  ## generate data row by row:
  N<-dim(fakedat)[1]
  rt<-rep(NA,N)
  for(i in 1:N){
    rt[i] <- rlnorm(1,alpha +
                      u[fakedat[i,]$subj,1] +
                      w[fakedat[i,]$item,1] +
                      (beta+u[fakedat[i,]$subj,2]+
                         w[fakedat[i,]$item,2])*fakedat$so[i],
                   sigma_e)}
  fakedat$rt<-rt
  fakedat$subj<-factor(fakedat$subj); fakedat$item<-factor(fakedat$item)
  fakedat}
```

Next, we extract the parameter means from the Bayesian model, and assemble the variance covariance matrices for the subject and item random effects.
 
```{r} 
sds<-colMeans(sd)
cors<-colMeans(cor)
sig<-mean(sigma$sigma)
Sigma_u<-diag(sds[3:4]^2)
Sigma_u[1,2]<-Sigma_u[2,1]<-cors[2]*sds[3]*sds[4]
Sigma_w<-diag(sds[1:2]^2)
Sigma_w[1,2]<-Sigma_w[2,1]<-cors[1]*sds[1]*sds[2]
```

Then, we run 50 simulations, computing the 95% credible interval of the OR processing cost effect.
Because this is a very time-consuming calculation, we are going to use previously computed values.

```{r eval=FALSE}
nsim<-50
betaquants<-matrix(rep(NA,nsim*2),ncol =2)
betameans<-matrix(rep(NA,nsim),ncol =2)

for(i in 1:nsim){
gg_fake<-gen_fake_lnorm(alpha=mean(alpha),
                        beta=mean(beta),
               Sigma_u=Sigma_u,Sigma_w=Sigma_w,
               sigma_e=sig)

m_gg_fake<-brm(rt~so + (1+so|subj) + (1+so|item),gg_fake,family=lognormal(),
    prior=priors,
    control = list(adapt_delta = 0.99,max_treedepth=15))
betapost<-posterior_samples(m_gg_fake)$b_so
betaquants[i,]<-quantile(betapost,prob=c(0.025,0.975))
betameans[i]<-mean(betapost)
}
save(betameans,
     file="data/truediscoverymeans.Rda")
save(betaquants,
     file="data/truediscoveryquants.Rda")
```

This simulation gives us an estimate of the proportion of times we would be able to detect an effect with 16 items and 42 subjects, assuming that the Grodner and Gibson data reflect a real difference between the two relative clause types. 

Assuming that we are willing to declare an effect just in case 0 is not included in the 95% credible interval of the effect, 
the above simulation shows that we would detect the effect in only half of the repeated experiments. 

\begin{verbatim}
> length(which(betaquants[,1]>0))/50
[1] 0.5
\end{verbatim}

Thus, the true discovery rate is quite low. One would want the true discovery rate to be at least 80%. 

We can also investigate the false discovery rate---the proportion of times we would declare that we found an effect, when there is none. In frequentist statistics, this is called Type I error. The only change needed in the above simulation is to set $\beta$ to 0, to reflect the assumption that there is no effect.

### Posterior predictive checks

Figure \ref{fig:ggpostpred} shows that we have reasonable coverage over the observed data.

```{r fig.cap="\\label{fig:ggpostpred}Posterior predictive check for the Grodner and Gibson data."}
pp_check(m_gg, nsamples = 100)+
  theme(text = element_text(size=16),
        legend.text=element_text(size=16))
```

## Example 2: Question-response accuracies (Logistic regression)

The @grodner data also has question response accuracies: 1 if the response to a question following the sentence was correct, 0 otherwise. We show only the relevant columns below:

```{r}
head(gge1crit[,c(1,2,3,8,11)])
```

